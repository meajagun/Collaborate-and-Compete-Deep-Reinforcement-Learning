{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Report - Collaboration & Competition\n",
    "\n",
    "### Overview\n",
    "\n",
    "This projects trains two (2) agents how to play Tennis. In the environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1 while if an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). After each episode, the rewards that each agent received are added (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores which yields a single **score** for each episode.\n",
    "\n",
    "The goal was for the agents to solve the environment and achive an average score of of +0.05 over 100 consecutive episodes.\n",
    "\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "Multi-Agent Deep Derterministic Policy Gradient (MADDPG) was the algorithm used to solve this environment. It was a modified version adopted from the Udacity Pendulum exercise where the DDPG function was modified for training two (2) seperate agents. \n",
    "\n",
    "MADDPG learning algorithm allows each agent to learn directly from the un-processed observation spaces without knowing the domain dynamic information. The general concept of this algorithm is to use policy and value based methods with Actor - Critic methods. \n",
    "\n",
    "The MADDPG function leverages on the DDPG funciton but this time for two (2) seperate agents and uses the policy based approach to find the optimal policy and maximize reward using gradient descent (Actor) while the value based approach estimates the value for future rewards (Critic). \n",
    "\n",
    "Important to note here is that with MADDPG, each agents critic is trained using the observations and states from both agents while the agents actors are trained with their seperate observations.\n",
    "\n",
    "The environment was solved by the agents getting an average score of 0.503 above the target score of 0.5 in 2235 episodes.\n",
    "\n",
    "The network architecture model had two Deep Neural Networks (DNN) one for the Actor and the other for the Critic. The both had three (3) linear layers with RELU as the activation function for each layer in the model.py file between the input state size and output actions size.\n",
    "\n",
    "A great challenge was having to run the model over 10 times before it finallyc converged. I tweaked the hyperparameters several times to no success. But i also realized that with the same model and hyperparameters, each training cycle yields entierly different results.\n",
    "\n",
    "\n",
    "##### The Learning algorithm for MADDPG uses the following:\n",
    "* ***Actor-Critic DNN method*** where the actor agent lerans how to estimate the optimal policy and the critic agent learns how to estimate the value of different state-action pairs.\n",
    "* ***Gradient Clipping*** apply gradient clipping after the backward pass to prevent exploding grdients when training the critic network.\n",
    "* ***Learn Interval and Update at Timesteps*** to allow the actor and critic agent to learn at intervals of 5 timesteps and improve the agents performance.\n",
    "* ***Experience replay*** to train through mini-batches from the replay buffer by sampling experiences uniformly at random from the replay memory and gather experiences from each agent.\n",
    "\n",
    "The hyperparameters used to train the MADDPG Agent were:\n",
    "* BUFFER_SIZE = int(1e6)  \n",
    "* BATCH_SIZE = 256        \n",
    "* LR_ACTOR = 1e-3         \n",
    "* LR_CRITIC = 1e-3        \n",
    "* WEIGHT_DECAY = 0        \n",
    "* LEARN_EVERY = 1         \n",
    "* LEARN_NUM = 5           \n",
    "* GAMMA = 0.99            \n",
    "* TAU = 2e-2              \n",
    "* OU_SIGMA = 0.2          \n",
    "* OU_THETA = 0.15         \n",
    "* EPS_START = 5.0         \n",
    "* EPS_EP_END = 300        \n",
    "* EPS_FINAL = 0           \n",
    "\n",
    "\n",
    "### Trained MADDPG Results\n",
    "\n",
    "The figure below shows the trained two (2) MADDPG Tennis Agent results with a moving average score. \n",
    "* The MADDPG Agent average score was 0.503 in 2235 episodes.\n",
    "\n",
    "![](maddpg_result.png)\n",
    "\n",
    "\n",
    "\n",
    "### Ideas For Future Network\n",
    "\n",
    "This project has utilized the Multi-Agent Deep Derterministic Policy Gradient (MADDPG). For further and future work, I plant to try out other algorithms to see how they perform in comparison to MADDPG:\n",
    "1. ***Proximal Policy Optimization (PPO)***\n",
    "2. ***Distributional Deterministic Policy Gradients (D4PG)***\n",
    "3. ***Add Prioritized Experience Replay to DDPG*** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
